{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYJxbPhXJRq61hBGiQU1MF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nayoung-Oh/ChatGPT_Team2/blob/Darae/generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDmYjwKeOLZa",
        "outputId": "d131366e-2b35-498c-a259-cfa5d8ea70ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def listdir_fullpath(d):\n",
        "    return [os.path.join(d, f) for f in os.listdir(d) if f.endswith('.json')]\n",
        "  \n",
        "def read_sentences(file):\n",
        "    with open(file, 'r', encoding='utf-8-sig') as f:\n",
        "        data = json.load(f)\n",
        "    sentence_data = []\n",
        "    for d in data['utterance']:\n",
        "        tmp = {}\n",
        "        tmp['dialect'] = d['dialect_form']\n",
        "        tmp['standard'] = d['standard_form']\n",
        "        tmp['dialect words'] = list(set([k['eojeol'] for k in d['eojeolList'] if k['isDialect']]))\n",
        "        tmp['standard words'] = list(set([k['standard'] for k in d['eojeolList'] if k['isDialect']]))\n",
        "        for w in tmp['dialect words']:\n",
        "            if w not in tmp['dialect']:\n",
        "                continue\n",
        "        if '(' in tmp['standard'] or '(' in tmp['dialect'] or '%' in tmp['dialect'] or '&' in tmp['dialect'] or '{' in tmp['dialect'] or '#' in tmp['standard']:\n",
        "          continue\n",
        "        if len(tmp['dialect words']) != 0:\n",
        "            sentence_data.append(tmp)\n",
        "    return sentence_data\n",
        "\n",
        "folder = listdir_fullpath('/content/gdrive/MyDrive/chatgpt/[라벨]제주도_학습용데이터_1')\n",
        "sentence_data = []\n",
        "for file in folder:\n",
        "    sentence_data.extend(read_sentences(file))\n",
        "one_dialect = []\n",
        "multiple_dialects = []\n",
        "length = 0\n",
        "for s in sentence_data:\n",
        "    length += len(s['dialect'])\n",
        "    length += len(s['standard'])\n",
        "    if len(s['dialect words']) > 3:\n",
        "        multiple_dialects.append(s)\n",
        "    else:\n",
        "        one_dialect.append(s)\n",
        "\n",
        "dialect_dict = set()\n",
        "for s in multiple_dialects:\n",
        "    dialect_dict.update(s['dialect words'])\n",
        "print(len(dialect_dict))\n",
        "print(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F__2WnJJ0dzk",
        "outputId": "e2af9115-1239-43a5-ca97-8d705d9c5005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4931\n",
            "834974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('result.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(sentence_data, f, indent='\\t', ensure_ascii=False)"
      ],
      "metadata": {
        "id": "mzAV8kJMAGm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}